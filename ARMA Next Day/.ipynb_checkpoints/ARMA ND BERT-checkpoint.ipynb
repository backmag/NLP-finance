{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange, tqdm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONEDRIVE_PATH = Path(r\"C:\\Users\\gusta\\Kidbrooke Advisory Ab\\KidbrookeOneDrive - Gustaf Backman exjobb\")\n",
    "\n",
    "# Read the financial data \n",
    "fin_path = ONEDRIVE_PATH / \"Input Data/stock_data.pkl\"\n",
    "fin_data = pd.read_pickle(fin_path)\n",
    "# Read from 2006-10-20 for next day preds or from 2006-10-19 for current day preds\n",
    "fin_data = fin_data.loc['2006-10-20' : '2013-11-22']\n",
    "\n",
    "news_path = ONEDRIVE_PATH / \"Input Data/financial_headlines_20061020-20131119.pkl\"\n",
    "news_data = pd.DataFrame(pd.read_pickle(news_path))\n",
    "news_data.set_index('date',inplace=True)\n",
    "\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dates = news_data.index.drop_duplicates()\n",
    "fin_dates = fin_data.index.drop_duplicates\n",
    "\n",
    "# Find the dates which are present in both the financial data and news data\n",
    "valid_dates = []\n",
    "for date in news_dates: \n",
    "    if date in fin_data.index: \n",
    "        valid_dates.append(date)\n",
    "        \n",
    "# Create targets for all 3 time series \n",
    "targets = pd.DataFrame(dtype='int8')\n",
    "prev_vals = fin_data.iloc[0]\n",
    "for i in range(len(valid_dates)): \n",
    "    y_temp = []\n",
    "    # If the value has increased since yesterday, y = 1\n",
    "    if prev_vals['1 YEAR'] < fin_data.iloc[i+1]['1 YEAR']: \n",
    "        y_temp.append(1)\n",
    "    else:\n",
    "        y_temp.append(0)\n",
    "        \n",
    "    if prev_vals['3 YEAR'] < fin_data.iloc[i+1]['3 YEAR']: \n",
    "        y_temp.append(1)\n",
    "    else:\n",
    "        y_temp.append(0)\n",
    "        \n",
    "    if prev_vals['S&P'] < fin_data.iloc[i+1]['S&P']: \n",
    "        y_temp.append(1)\n",
    "    else:\n",
    "        y_temp.append(0)\n",
    "    targets = targets.append({'1 YEAR':y_temp[0], '3 YEAR':y_temp[1], 'S&P':y_temp[2]}, ignore_index=True)\n",
    "    prev_vals = fin_data.iloc[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████████▎                                         | 873/1846 [01:21<01:11, 13.59it/s]"
     ]
    }
   ],
   "source": [
    "# Concatenate the news for each day in the valid indices, i.e. the indices that are present in both \n",
    "# the financial data and the news data \n",
    "texts = []\n",
    "for ind,date in enumerate(tqdm(valid_dates)): \n",
    "    temp_texts = \"\"\n",
    "    for i,text in enumerate(news_data.loc[date]['title']): \n",
    "        if i < len(news_data.loc[date]['title']) - 1:\n",
    "            if any(char in text[-2:] for char in ['!','?']):\n",
    "                temp_texts += text + \" \"\n",
    "            else:\n",
    "                temp_texts += text + \". \"\n",
    "        else:\n",
    "            if any(char in text[-2:] for char in ['!','?']):\n",
    "                temp_texts += text + \"\"\n",
    "            else:\n",
    "                temp_texts += text + \".\"\n",
    "    texts.append(temp_texts)\n",
    "texts = np.asarray(texts)\n",
    "print(\"Length of texts:\",len(texts))\n",
    "print(\"Length of targets:\",len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "bert_module =  hub.Module(bert_path)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                      tokenization_info[\"do_lower_case\"],])\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for titles in tqdm(texts):\n",
    "    tokens.append(tokenizer.tokenize(titles.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Maximum sequence length is 512, check lengths on dataset\n",
    "lengths = np.asarray([len(t) for t in tokens])\n",
    "n = len(lengths)\n",
    "plt.hist(lengths,20)\n",
    "plt.show()\n",
    "print(\"Ratio under 512:\",sum(lengths < 512) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "\n",
    "input_ids = np.zeros((n,max_length))\n",
    "input_masks = np.zeros((n,max_length))\n",
    "segment_ids = np.zeros((n,max_length))\n",
    "\n",
    "for i in trange(n):\n",
    "    tok = tokens[i]\n",
    "    if len(tok) > max_length - 2:\n",
    "        tok = tok[:max_length - 2]\n",
    "    tok = ['[CLS]'] + tok + ['[SEP]']\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tok)\n",
    "    \n",
    "    if len(token_ids) > max_length:\n",
    "        token_ids = token_ids[:max_length]\n",
    "        mask = [1] * max_length\n",
    "    else: \n",
    "        mask = [1] * len(token_ids) + [0] * (max_length - len(token_ids))\n",
    "        token_ids = token_ids + [0] * (max_length - len(token_ids))\n",
    "    \n",
    "    current_segment_id = 0\n",
    "    segments = []\n",
    "    for t in tok:\n",
    "        segments.append(current_segment_id)\n",
    "        if t == '[SEP]':\n",
    "            current_segment_id = 1\n",
    "    while len(segments) < max_length: \n",
    "        segments.append(0)\n",
    "    assert len(token_ids) == max_length, \"Wrong length of ids\"\n",
    "    assert len(segments) == max_length, \"Wrong length of segments.\"\n",
    "    assert len(mask) == max_length, \"Wrong length of masks.\"\n",
    "    \n",
    "    input_ids[i] = np.asarray(token_ids)\n",
    "    segment_ids[i] = segments\n",
    "    input_masks[i] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indices_1yr = np.where(targets['1 YEAR'] == 0)[0]\n",
    "one_indices_1yr = np.where(targets['1 YEAR'] == 1)[0]\n",
    "\n",
    "zero_indices_3yr = np.where(targets['3 YEAR'] == 0)[0]\n",
    "one_indices_3yr = np.where(targets['3 YEAR'] == 1)[0]\n",
    "\n",
    "zero_indices_sp = np.where(targets['S&P'] == 0)[0]\n",
    "one_indices_sp = np.where(targets['S&P'] == 1)[0]\n",
    "\n",
    "print(\" ----- 1 YEAR RATE -----\")\n",
    "print(\"Ratio neg labels 1 year rate:\",len(zero_indices_1yr) / len(targets))\n",
    "print(\"Neg labels:\", len(zero_indices_1yr), \"Pos labels:\",len(one_indices_1yr))\n",
    "print(\" ----- 3 YEAR RATE -----\")\n",
    "print(\"Ratio neg labels 3 year rate:\",len(zero_indices_3yr) / len(targets))\n",
    "print(\"Neg labels:\", len(zero_indices_3yr), \"Pos labels:\",len(one_indices_3yr))\n",
    "print(\" -----     S&P     -----\")\n",
    "print(\"Ratio neg labels S&P:\",len(zero_indices_sp) / len(targets))\n",
    "print(\"Neg labels:\", len(zero_indices_sp), \"Pos labels:\",len(one_indices_sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1200\n",
    "\n",
    "np.random.shuffle(zero_indices_1yr)\n",
    "np.random.shuffle(one_indices_1yr)\n",
    "part_zeros_1yr = len(zero_indices_1yr) / n\n",
    "train_indices_1yr = np.zeros((n_train),dtype=int)\n",
    "train_indices_1yr[:round(part_zeros_1yr * n_train)] = zero_indices_1yr[:round(part_zeros_1yr * n_train)]\n",
    "train_indices_1yr[round(part_zeros_1yr * n_train):] = one_indices_1yr[:round((1 - part_zeros_1yr) * n_train)]\n",
    "np.random.shuffle(train_indices_1yr)\n",
    "# All indices not in train is in test. \n",
    "test_indices_1yr = np.setdiff1d(np.arange(len(targets)), train_indices_1yr)\n",
    "np.random.shuffle(test_indices_1yr)\n",
    "\n",
    "np.random.shuffle(zero_indices_3yr)\n",
    "np.random.shuffle(one_indices_3yr)\n",
    "part_zeros_3yr = len(zero_indices_3yr) / n\n",
    "train_indices_3yr = np.zeros((n_train),dtype=int)\n",
    "train_indices_3yr[:round(part_zeros_3yr * n_train)] = zero_indices_3yr[:round(part_zeros_3yr * n_train)]\n",
    "train_indices_3yr[round(part_zeros_3yr * n_train):] = one_indices_3yr[:round((1 - part_zeros_3yr) * n_train)]\n",
    "np.random.shuffle(train_indices_3yr)\n",
    "# All indices not in train is in test. \n",
    "test_indices_3yr = np.setdiff1d(np.arange(len(targets)), train_indices_3yr)\n",
    "np.random.shuffle(test_indices_3yr)\n",
    "\n",
    "np.random.shuffle(zero_indices_sp)\n",
    "np.random.shuffle(one_indices_sp)\n",
    "part_zeros_sp = len(zero_indices_sp) / n\n",
    "train_indices_sp = np.zeros((n_train),dtype=int)\n",
    "train_indices_sp[:round(part_zeros_sp * n_train)] = zero_indices_sp[:round(part_zeros_sp * n_train)]\n",
    "train_indices_sp[round(part_zeros_sp * n_train):] = one_indices_sp[:round((1 - part_zeros_sp) * n_train)]\n",
    "np.random.shuffle(train_indices_sp)\n",
    "# All indices not in train is in test. \n",
    "test_indices_sp = np.setdiff1d(np.arange(len(targets)), train_indices_sp)\n",
    "np.random.shuffle(test_indices_sp)\n",
    "\n",
    "train_ids_1yr = input_ids[train_indices_1yr]\n",
    "train_segs_1yr = segment_ids[train_indices_1yr]\n",
    "train_masks_1yr = input_masks[train_indices_1yr]\n",
    "y_train_1yr = targets.loc[train_indices_1yr]['1 YEAR'].values\n",
    "\n",
    "test_ids_1yr = input_ids[test_indices_1yr]\n",
    "test_segs_1yr = segment_ids[test_indices_1yr]\n",
    "test_masks_1yr = input_masks[test_indices_1yr]\n",
    "y_test_1yr = targets.loc[test_indices_1yr]['1 YEAR'].values\n",
    "\n",
    "train_ids_3yr = input_ids[train_indices_3yr]\n",
    "train_segs_3yr = segment_ids[train_indices_3yr]\n",
    "train_masks_3yr = input_masks[train_indices_3yr]\n",
    "y_train_3yr = targets.loc[train_indices_3yr]['3 YEAR'].values\n",
    "\n",
    "test_ids_3yr = input_ids[test_indices_3yr]\n",
    "test_segs_3yr = segment_ids[test_indices_3yr]\n",
    "test_masks_3yr = input_masks[test_indices_3yr]\n",
    "y_test_3yr = targets.loc[test_indices_3yr]['3 YEAR'].values\n",
    "\n",
    "train_ids_sp = input_ids[train_indices_sp]\n",
    "train_segs_sp = segment_ids[train_indices_sp]\n",
    "train_masks_sp = input_masks[train_indices_sp]\n",
    "y_train_sp = targets.loc[train_indices_sp]['S&P'].values\n",
    "\n",
    "test_ids_sp = input_ids[test_indices_sp]\n",
    "test_segs_sp = segment_ids[test_indices_sp]\n",
    "test_masks_sp = input_masks[test_indices_sp]\n",
    "y_test_sp = targets.loc[test_indices_sp]['S&P'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" ----- 1 YEAR RATE -----\")\n",
    "print(\"Train data label ratio (0 / 1):\",np.sum(y_train_1yr == 0),\"/\", np.sum(y_train_1yr == 1))\n",
    "print(\"Test data label ratio (0 / 1):\",np.sum(y_test_1yr == 0),\"/\", np.sum(y_test_1yr == 1))\n",
    "\n",
    "print(\" ----- 3 YEAR RATE -----\")\n",
    "print(\"Train data label ratio (0 / 1):\",np.sum(y_train_3yr == 0),\"/\", np.sum(y_train_3yr == 1))\n",
    "print(\"Test data label ratio (0 / 1):\",np.sum(y_test_3yr == 0),\"/\", np.sum(y_test_3yr == 1))\n",
    "print(\" -----     S&P     -----\")\n",
    "print(\"Train data label ratio (0 / 1):\",np.sum(y_train_sp == 0),\"/\", np.sum(y_train_sp == 1))\n",
    "print(\"Test data label ratio (0 / 1):\",np.sum(y_test_sp == 0),\"/\", np.sum(y_test_sp == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dense_nodes=256, dropout_rate=0.1, learning_rate=2e-5):\n",
    "    adam = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    in_ids = keras.layers.Input(shape=(max_length,), dtype=tf.int32,name='input_id')\n",
    "    in_mask = keras.layers.Input(shape=(max_length,),dtype=tf.int32,name='input_mask')\n",
    "    in_seg = keras.layers.Input(shape=(max_length,),dtype=tf.int32, name='input_segment')\n",
    "\n",
    "    bert_inputs = [in_ids, in_mask, in_seg]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=2,pooling='first')(bert_inputs)\n",
    "    if dense_nodes == 0: \n",
    "        output_layer = keras.layers.Dense(1, activation='sigmoid')(bert_output)\n",
    "        model = keras.models.Model(inputs=bert_inputs, outputs=output_layer)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "        return model \n",
    "    dense_layer = keras.layers.Dense(dense_nodes, activation='relu')(bert_output)\n",
    "    dropout_layer = keras.layers.Dropout(dropout_rate)(dense_layer)\n",
    "    output_layer = keras.layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
    "    \n",
    "    model = keras.models.Model(inputs=bert_inputs, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def init_sess(sess): \n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "    \n",
    "def cont_training(model,x,y,epochs,history, validation_split=0.2): \n",
    "    temp_hist = model.fit(x,y,epochs=epochs,batch_size=32,validation_split=validation_split)\n",
    "    for metric in history.history: \n",
    "        history.history[metric].append(temp_hist.history[metric][0])\n",
    "    return history\n",
    "    \n",
    "def plot_results_nn(history, model,test_ids,test_masks, test_segs,y_test,save=False, name=''):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'],label='Loss')\n",
    "    plt.plot(history.history['val_loss'],label='Val loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(history.history['acc'],label='Acc')\n",
    "    plt.plot(history.history['val_acc'],label='Val acc')\n",
    "    plt.legend()\n",
    "    if save: \n",
    "        plt.savefig(\"Figs BERT/\" + name + \"history.jpg\")\n",
    "    plt.show() \n",
    "    y_test_pred = np.round(model.predict([test_ids, test_masks, test_segs]))\n",
    "    plt.subplot(121)\n",
    "    plt.bar([0,1],[np.sum(y_test == 0), np.sum(y_test == 1)], label='Actual test dist')\n",
    "    plt.legend()\n",
    "    plt.xticks([0, 1])\n",
    "    plt.subplot(122)\n",
    "    plt.bar([0,1],[np.sum(y_test_pred == 0), np.sum(y_test_pred == 1)], label='Pred test dist')\n",
    "    plt.legend()\n",
    "    plt.xticks([0, 1])\n",
    "    plt.show()\n",
    "    conf_mat = confusion_matrix(y_test,y_test_pred,normalize='pred')\n",
    "    ax = sn.heatmap(conf_mat,)\n",
    "    ax.set_ylabel(\"True values\")\n",
    "    ax.set_xlabel(\"Predicted values\")\n",
    "    if save: \n",
    "        plt.savefig(\"Figs BERT/\" + name + \"confmat.jpg\")\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"Test accuracy:\",round(np.sum(y_test == y_test_pred[:,0]) / len(y_test),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid layer on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1yr = create_model(dense_nodes=0)\n",
    "init_sess(sess)\n",
    "history_1yr = model_1yr.fit([train_ids_1yr, train_masks_1yr, train_segs_1yr], y_train_1yr,\n",
    "                            validation_split=0.2,\n",
    "                            epochs=1000,\n",
    "                            batch_size=16,\n",
    "                            callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_nn(history_1yr, model_1yr, test_ids_1yr, test_masks_1yr, test_segs_1yr, \n",
    "                y_test_1yr,\n",
    "                save=True,\n",
    "                name='1yr_sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_3yr = create_model(dense_nodes=0)\n",
    "init_sess(sess)\n",
    "history_3yr = model_3yr.fit([train_ids_3yr, train_masks_3yr, train_segs_3yr], y_train_3yr, \n",
    "                            epochs=10000,\n",
    "                            batch_size=32,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_nn(history_3yr, model_3yr, test_ids_3yr, test_masks_3yr, test_segs_3yr, \n",
    "                y_test_3yr,\n",
    "                save=True,\n",
    "                name='3yr_sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp = create_model(dense_nodes=0) \n",
    "init_sess(sess)\n",
    "history_sp = model_sp.fit([train_ids_sp, train_masks_sp, train_segs_sp], y_train_sp,\n",
    "                          epochs=10000,\n",
    "                          batch_size=32,\n",
    "                          validation_split=0.2,\n",
    "                          callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_nn(history_sp, model_sp, test_ids_sp, test_masks_sp, test_segs_sp, \n",
    "                y_test_sp,\n",
    "                save=True,\n",
    "                name='sp_sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP on top\n",
    "Same config for all models, dense nodes=256, dropout_rate = 0.1 and 2e-5 learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1yr_mlp = create_model()\n",
    "init_sess(sess)\n",
    "history_1yr_mlp = model_1yr_mlp.fit([train_ids_1yr, train_masks_1yr, train_segs_1yr], y_train_1yr,\n",
    "                            validation_split=0.2,\n",
    "                            epochs=1000,\n",
    "                            batch_size=16,\n",
    "                            callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])\n",
    "\n",
    "plot_results_nn(history_1yr_mlp, model_1yr_mlp, test_ids_1yr, test_masks_1yr, test_segs_1yr, \n",
    "                y_test_1yr,\n",
    "                save=True,\n",
    "                name='1yr_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3yr_mlp = create_model(dense_nodes=0)\n",
    "init_sess(sess)\n",
    "history_3yr_mlp = model_3yr_mlp.fit([train_ids_3yr, train_masks_3yr, train_segs_3yr], y_train_3yr,\n",
    "                            validation_split=0.2,\n",
    "                            epochs=1000,\n",
    "                            batch_size=16,\n",
    "                            callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])\n",
    "\n",
    "plot_results_nn(history_3yr_mlp, model_3yr_mlp, test_ids_3yr, test_masks_3yr, test_segs_3yr, \n",
    "                y_test_3yr,\n",
    "                save=True,\n",
    "                name='3yr_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp_mlp = create_model(dense_nodes=0)\n",
    "init_sess(sess)\n",
    "history_sp_mlp = model_sp_mlp.fit([train_ids_sp, train_masks_sp, train_segs_sp], y_train_sp,\n",
    "                            validation_split=0.2,\n",
    "                            epochs=1000,\n",
    "                            batch_size=16,\n",
    "                            callbacks=[EarlyStopping(monitor='val_acc', patience=2, restore_best_weights=True)])\n",
    "\n",
    "plot_results_nn(history_sp_mlp, model_sp_mlp, test_ids_sp, test_masks_sp, test_segs_sp, \n",
    "                y_test_sp,\n",
    "                save=True,\n",
    "                name='sp_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
