{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, LSTM, GRU, SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_model, model_to_dot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from collections import deque\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "%run ARIMA-creator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, arima_data, fin_data, lookback, delay, indices, batch_size=32):\n",
    "    \"\"\" Creates a generator for the given data. Returns each \n",
    "    batch with separate word embeddings and ARIMA-predictions. \n",
    "    \n",
    "    Parameters: \n",
    "    data - Sorted data of size (n,d)\n",
    "    arima_data - Predictions from the arima-models \n",
    "    lookback - How far back to include data (days)\n",
    "    delay - How far ahead to predict (days)\n",
    "    indices - Shuffled indices to include, where the order is followed\n",
    "    batch_size - How many samples to return each call \n",
    "    \n",
    "    Returns: \n",
    "    samples_words - Word-related data \n",
    "    samples_arima - The ARIMA-predicted future value \n",
    "    targets - The value of the times series delay days ahead\n",
    "    \n",
    "    \"\"\"\n",
    "    min_index = lookback\n",
    "    max_index = len(data) - delay - 1\n",
    "    current_batch_start = 0\n",
    "    while True:\n",
    "        if current_batch_start > len(indices): \n",
    "            current_batch_start = 0\n",
    "        rows = indices[current_batch_start:min(current_batch_start+batch_size,max_index)]\n",
    "        current_batch_start += batch_size\n",
    "        samples_words = np.zeros((len(rows), lookback, data.shape[-1]))\n",
    "        samples_arima = np.zeros(len(rows),)\n",
    "        targets = np.zeros(len(rows),)\n",
    "        dates = []\n",
    "        for i, index in enumerate(rows): \n",
    "            date = arima_data.index[index]\n",
    "            dates.append(date)\n",
    "            target_date = date + timedelta(days=delay)     # Does this always give a viable date? \n",
    "            if (index > min_index) and (index < max_index):\n",
    "                samples_words[i] = data[range(index - lookback,index)]\n",
    "                samples_arima[i] = arima_data[date]\n",
    "                targets[i] = fin_data.loc[target_date]\n",
    "        yield [samples_words, samples_arima], targets\n",
    "\n",
    "        \n",
    "def concat_daily(df):\n",
    "    conc = pd.DataFrame()\n",
    "    for i in set(df.index):\n",
    "        concat_str = ''\n",
    "        for title in df.loc[i]['title']:\n",
    "            concat_str += \" \" + title\n",
    "        conc = conc.append({'date':i, 'title':concat_str},ignore_index=True)\n",
    "    conc.set_index('date',inplace=True)\n",
    "    conc = conc.sort_values('date')\n",
    "    return conc\n",
    "\n",
    "def pad_data(seq,maxlen):\n",
    "    data = np.zeros((len(seq), maxlen),dtype=int)\n",
    "    for i,s in enumerate(seq): \n",
    "        if len(s) <= maxlen: \n",
    "            data[i,:len(s)] = s\n",
    "        else: \n",
    "            s = np.array(s)\n",
    "            indices = np.sort(np.random.choice(len(s),maxlen,replace=False))\n",
    "            data[i,:] = s[indices]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters to be saved in a config file later. \n",
    "par = {\n",
    "    'embed_dim': 100,    # Dimensions to use for the word embedding\n",
    "    'vocab_part': 0.6,   # How large part of the total vocabulary to include\n",
    "    'lookback': 20,      # How far back to collect data in the recurrent layer (days)\n",
    "    'delay': 5,          # How far ahead to predict data (days)\n",
    "    'batch_size': 10,    # Batch size used in generator\n",
    "    'p': 1,              # Order of the AR-part of the model\n",
    "    'd': 1,              # Integrated order\n",
    "    'q': 1,              # Included moving average terms \n",
    "    'train_part' : 0.6,  # Part of data to be used for training\n",
    "    'val_part' : 0.2,    # Part of data to be used for validation\n",
    "    'test_part' : 0.2,   # Part of data to be used for testing\n",
    "    'series': '1 YEAR'   # What series we currently want to predict, '1 YEAR', '3 YEAR' or 'S&P'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "news_path = Path(os.getcwd()) / \"Datasets/data/financial_headlines_20061020-20131119.pkl\"\n",
    "stock_path = Path(os.getcwd()) / \"Datasets/data/stock_data.pkl\"\n",
    "data = pd.DataFrame(pd.read_pickle(news_path))\n",
    "data.set_index('date',inplace=True)\n",
    "stock_data = pd.DataFrame(pd.read_pickle(stock_path))\n",
    "data = concat_daily(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process text data \n",
    "\n",
    "len_words = [len(title) for title in data['title'].values]\n",
    "mean_words = np.mean(len_words)\n",
    "std_words = np.std(len_words)\n",
    "\n",
    "sent_len = int(mean_words + 2 * std_words)  # THIS MAKES IT SLOW!?                                                                                                                                                        \n",
    "sent_len = 500\n",
    "\n",
    "\n",
    "# Update environment variable config\n",
    "par.update({'input_dim': sent_len})\n",
    "par.update({'start_date' : data.index[0]})\n",
    "par.update({'end_date' : data.index[-1]})\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=None)  # Tokenize without limitation first, just because it's a cheap way \n",
    "tokenizer.fit_on_texts(data['title'])  # of calculating the number of unique words in the data\n",
    "par.update({'vocab_size': len(tokenizer.word_index)})\n",
    "\n",
    "tokenizer = Tokenizer(num_words=par['vocab_size'] * par['vocab_part'])\n",
    "\n",
    "tokenizer.fit_on_texts(data['title'])\n",
    "sequences = tokenizer.texts_to_sequences(data['title'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "par.update({'vocab_size': len(word_index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length sent_len. End-padded.\n",
    "# If a sequence is longer than sent_len, words are randomly sampled.\n",
    "text_data = pad_data(sequences,sent_len)\n",
    "\n",
    "# Extract financial data\n",
    "stock_data = stock_data[par['start_date'] : par['end_date']]\n",
    "\n",
    "# Add indicies which are present in the news data but not in the \n",
    "# financial data and interpolate missing values \n",
    "stock_data = stock_data.reindex(data.index.drop_duplicates())\n",
    "stock_data = stock_data.interpolate()\n",
    "\n",
    "# Normalize the financial data to [0,1]\n",
    "fin_stats = pd.DataFrame(columns=['min','max'])\n",
    "for col in stock_data: \n",
    "    minimum = min(stock_data[col])\n",
    "    maximum = max(stock_data[col])\n",
    "    fin_stats = fin_stats.append({'min':minimum, 'max':maximum},ignore_index=True)\n",
    "    stock_data[col] = [(row - minimum) / (maximum - minimum) for row in stock_data[col]]\n",
    "fin_stats.index = stock_data.columns\n",
    "\n",
    "# Concatinate all data to one dataframe \n",
    "data = pd.DataFrame()\n",
    "for text in text_data:\n",
    "    data = data.append({'WORDS':text},ignore_index=True)\n",
    "data['DATE'] = stock_data.index\n",
    "data.set_index('DATE',inplace=True)\n",
    "for col in stock_data: \n",
    "    data[col] = stock_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded previously constructed models.\n"
     ]
    }
   ],
   "source": [
    "# Check if the ARIMA-models has been previously calculated for this config\n",
    "# or if they have to be constructed\n",
    "path = Path(f\"./Models/ARIMA/all_mods_del{par['delay']}.pkl\")\n",
    "if os.path.exists(path):\n",
    "    # Load model if if already exists\n",
    "    ARIMA_models = pd.read_pickle(path)    \n",
    "    print(\"Found and loaded previously constructed models.\")\n",
    "else: \n",
    "    # Fit ARIMA-models to all of the dates in the training data \n",
    "    print(\"Model not found, fitting models...\")\n",
    "    ARIMA_models = fit_all_models(par, data)\n",
    "    ARIMA_models.to_pickle(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "arima_preds = predict_arima(ARIMA_models, par['delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 YEAR</th>\n",
       "      <th>3 YEAR</th>\n",
       "      <th>S&amp;P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2006-11-10</td>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.885219</td>\n",
       "      <td>0.486327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-11</td>\n",
       "      <td>0.978997</td>\n",
       "      <td>0.894900</td>\n",
       "      <td>0.487505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-12</td>\n",
       "      <td>0.979915</td>\n",
       "      <td>0.894458</td>\n",
       "      <td>0.488468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-13</td>\n",
       "      <td>0.981899</td>\n",
       "      <td>0.898712</td>\n",
       "      <td>0.489512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-14</td>\n",
       "      <td>0.975154</td>\n",
       "      <td>0.887344</td>\n",
       "      <td>0.497077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-15</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-16</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.058758</td>\n",
       "      <td>0.998117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-17</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.057422</td>\n",
       "      <td>0.996633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-18</td>\n",
       "      <td>0.008061</td>\n",
       "      <td>0.056013</td>\n",
       "      <td>0.995077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-19</td>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.060271</td>\n",
       "      <td>0.992574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2561 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1 YEAR    3 YEAR       S&P\n",
       "date                                    \n",
       "2006-11-10  0.975805  0.885219  0.486327\n",
       "2006-11-11  0.978997  0.894900  0.487505\n",
       "2006-11-12  0.979915  0.894458  0.488468\n",
       "2006-11-13  0.981899  0.898712  0.489512\n",
       "2006-11-14  0.975154  0.887344  0.497077\n",
       "...              ...       ...       ...\n",
       "2013-11-15  0.008057  0.060185  0.999512\n",
       "2013-11-16  0.008060  0.058758  0.998117\n",
       "2013-11-17  0.008060  0.057422  0.996633\n",
       "2013-11-18  0.008061  0.056013  0.995077\n",
       "2013-11-19  0.010248  0.060271  0.992574\n",
       "\n",
       "[2561 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arima_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_data = []\n",
    "sequence_dates = []\n",
    "prev_data = deque(maxlen=par['lookback'])\n",
    "for i,row in enumerate(data['WORDS']): \n",
    "    prev_data.append(row)\n",
    "    if len(prev_data) == par['lookback']:\n",
    "        sequential_data.append(np.array(prev_data))\n",
    "        sequence_dates.append(data.index[i])\n",
    "sequential_data = np.asarray(sequential_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-51f969a8a13c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# there isn't enough data to make a prediction, varies depending on lookback and delay.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marima_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Add the predicitons to the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# Drop the values not in arima_preds. This is just values in the beginning where\n",
    "# there isn't enough data to make a prediction, varies depending on lookback and delay. \n",
    "for date in data.index: \n",
    "    if not date in arima_preds.index : data.drop(index=date,inplace=True)\n",
    "        \n",
    "# Add the predicitons to the training data \n",
    "data['1 YEAR PRED'] = arima_preds['1 YEAR'].values\n",
    "data['3 YEAR PRED'] = arima_preds['3 YEAR'].values\n",
    "data['S&P PRED'] = arima_preds['S&P'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the target data (the actual rates) so that each row has a target rate 'delay' days in the future\n",
    "data['1 YEAR'] = data['1 YEAR'].shift(-par['delay'])\n",
    "data['3 YEAR'] = data['3 YEAR'].shift(-par['delay'])\n",
    "data['S&P'] = data['S&P'].shift(-par['delay'])\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_preds = pd.DataFrame(np.zeros(2556))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2556, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arima_preds.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_rows = []\n",
    "for i,date in enumerate(sequence_dates): \n",
    "    if date not in data.index: del_rows.append(i)\n",
    "keep_rows = np.setdiff1d(np.arange(len(sequential_data)),del_rows)\n",
    "sequential_data = sequential_data[keep_rows]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training, validation and test segments. \n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Here we extract the time series specified in 'par' (for some reason...)\n",
    "words = np.array([row for row in data['WORDS'].values])\n",
    "arima_preds = np.array([row for row in data[par['series'] + ' PRED'].values])\n",
    "targets = np.array([row for row in data[par['series']].values])\n",
    "\n",
    "\n",
    "training_samples = round(par['train_part'] * len(data))\n",
    "validation_samples = round(par['val_part'] * len(data))\n",
    "test_samples = round(par['test_part'] * len(data))\n",
    "\n",
    "train_words = words[:training_samples - par['delay']]\n",
    "train_seq = sequential_data[:training_samples - par['delay']]\n",
    "train_arima = arima_preds[:training_samples - par['delay']]\n",
    "train_targets = targets[:training_samples - par['delay']]\n",
    "\n",
    "val_words = words[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_seq = sequential_data[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_arima = arima_preds[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_targets = targets[training_samples:training_samples + validation_samples - par['delay']]\n",
    "\n",
    "test_words = words[-test_samples:]\n",
    "test_seq = sequential_data[-test_samples:]\n",
    "test_arima = arima_preds[-test_samples:]\n",
    "test_targets = targets[-test_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data \n",
    "train_indices = np.arange(len(train_targets))\n",
    "random.shuffle(train_indices)\n",
    "train_words = train_words[train_indices]\n",
    "train_seq = train_seq[train_indices]\n",
    "train_arima = train_arima[train_indices]\n",
    "train_targets = train_targets[train_indices]\n",
    "\n",
    "val_indices = np.arange(len(val_targets))\n",
    "val_words = val_words[val_indices]\n",
    "val_seq = val_seq[val_indices]\n",
    "val_arima = val_arima[val_indices]\n",
    "val_targets = val_targets[val_indices]\n",
    "\n",
    "test_indices = np.arange(len(test_targets))\n",
    "test_words = test_words[test_indices]\n",
    "test_seq = test_seq[test_indices]\n",
    "test_arima = test_arima[test_indices]\n",
    "test_targets = test_targets[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging folder for TensorBoard\n",
    "logdir=Path(\"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_dim': 100,\n",
       " 'vocab_part': 0.6,\n",
       " 'lookback': 20,\n",
       " 'delay': 5,\n",
       " 'batch_size': 10,\n",
       " 'p': 1,\n",
       " 'd': 1,\n",
       " 'q': 1,\n",
       " 'train_part': 0.6,\n",
       " 'val_part': 0.2,\n",
       " 'test_part': 0.2,\n",
       " 'series': '1 YEAR',\n",
       " 'input_dim': 500,\n",
       " 'start_date': Timestamp('2006-10-20 00:00:00'),\n",
       " 'end_date': Timestamp('2013-11-19 00:00:00'),\n",
       " 'vocab_size': 25664}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Text_Input (InputLayer)         [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding_Layer (Embedding)     (None, 500, 100)     2566400     Text_Input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 10)           4440        Embedding_Layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 10)           0           lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ARIMA_input (InputLayer)        [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 11)           0           dropout_2[0][0]                  \n",
      "                                                                 ARIMA_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, 64)           768         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Main_Output (Dense)             (None, 1)            65          Dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,571,673\n",
      "Trainable params: 2,571,673\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network\n",
    "text_inputs = Input(shape=(par['input_dim']), name='Text_Input')\n",
    "\n",
    "# Layer for word embedding\n",
    "embedded_layer = Embedding(input_dim=par['vocab_size'],\n",
    "                           output_dim=par['embed_dim'],\n",
    "                           input_length=(par['input_dim']), name='Embedding_Layer')(text_inputs)\n",
    "\n",
    "# LSTM-layer over the embedding layer \n",
    "lstm_out = LSTM(10)(embedded_layer)\n",
    "dropout = Dropout(0.2)(lstm_out)\n",
    "\n",
    "# Input from an ARIMA-model independently fitted to the training data. \n",
    "ARIMA_input = Input(shape=(1,), name='ARIMA_input')\n",
    "\n",
    "# Merging the ARIMA-input and the input from the LSTM-layer. \n",
    "hidden = keras.layers.concatenate([dropout, ARIMA_input])\n",
    "\n",
    "# Stack of dense layers \n",
    "#hidden = Dense(64, activation='relu',name='Dense_1')(hidden)\n",
    "#hidden = Dropout(0.2)(hidden)\n",
    "\n",
    "hidden = Dense(64, activation='relu',name='Dense_2')(hidden)\n",
    "#hidden = Dropout(0.2)(hidden)\n",
    "\n",
    "# Main output of the model\n",
    "main_output = Dense(1,activation='linear',name='Main_Output')(hidden)\n",
    "\n",
    "model = Model(inputs=[text_inputs, ARIMA_input],outputs=[main_output])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='mse')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORDS</th>\n",
       "      <th>1 YEAR</th>\n",
       "      <th>3 YEAR</th>\n",
       "      <th>S&amp;P</th>\n",
       "      <th>1 YEAR PRED</th>\n",
       "      <th>3 YEAR PRED</th>\n",
       "      <th>S&amp;P PRED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2006-11-10</td>\n",
       "      <td>[53, 309, 1526, 21, 3541, 6942, 18, 19, 5891, ...</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.909278</td>\n",
       "      <td>0.495985</td>\n",
       "      <td>0.975805</td>\n",
       "      <td>0.885219</td>\n",
       "      <td>0.486327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-11</td>\n",
       "      <td>[695, 713, 1364, 81, 1003, 513, 236, 507, 713,...</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.921649</td>\n",
       "      <td>0.498327</td>\n",
       "      <td>0.978997</td>\n",
       "      <td>0.894900</td>\n",
       "      <td>0.487505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-12</td>\n",
       "      <td>[50, 1, 498, 150, 241, 508, 770, 285, 1483, 5,...</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.905155</td>\n",
       "      <td>0.499378</td>\n",
       "      <td>0.979915</td>\n",
       "      <td>0.894458</td>\n",
       "      <td>0.488468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-13</td>\n",
       "      <td>[6, 4, 389, 12839, 60, 1243, 129, 220, 1301, 1...</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.904467</td>\n",
       "      <td>0.499215</td>\n",
       "      <td>0.981899</td>\n",
       "      <td>0.898712</td>\n",
       "      <td>0.489512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2006-11-14</td>\n",
       "      <td>[6929, 41, 6, 4, 1763, 1297, 95, 241, 14, 77, ...</td>\n",
       "      <td>0.980159</td>\n",
       "      <td>0.903780</td>\n",
       "      <td>0.499051</td>\n",
       "      <td>0.975154</td>\n",
       "      <td>0.887344</td>\n",
       "      <td>0.497077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-10</td>\n",
       "      <td>[120, 1039, 3, 523, 9645, 12265, 10, 820, 848,...</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0.068606</td>\n",
       "      <td>0.977189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-11</td>\n",
       "      <td>[4936, 2, 59, 29, 977, 278, 934, 510, 66, 413,...</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.060481</td>\n",
       "      <td>0.998156</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.068291</td>\n",
       "      <td>0.977625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>[295, 297, 2696, 1145, 1602, 529, 70, 1602, 55...</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.059107</td>\n",
       "      <td>0.996312</td>\n",
       "      <td>0.008256</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>0.974523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-13</td>\n",
       "      <td>[467, 552, 1, 872, 269, 1972, 2, 1124, 2, 1103...</td>\n",
       "      <td>0.009921</td>\n",
       "      <td>0.057732</td>\n",
       "      <td>0.994468</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.066040</td>\n",
       "      <td>0.986115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2013-11-14</td>\n",
       "      <td>[533, 2877, 3055, 193, 71, 11, 232, 16, 318, 1...</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.061856</td>\n",
       "      <td>0.991664</td>\n",
       "      <td>0.008066</td>\n",
       "      <td>0.056115</td>\n",
       "      <td>0.993311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2556 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        WORDS    1 YEAR  \\\n",
       "DATE                                                                      \n",
       "2006-11-10  [53, 309, 1526, 21, 3541, 6942, 18, 19, 5891, ...  0.984127   \n",
       "2006-11-11  [695, 713, 1364, 81, 1003, 513, 236, 507, 713,...  0.988095   \n",
       "2006-11-12  [50, 1, 498, 150, 241, 508, 770, 285, 1483, 5,...  0.980159   \n",
       "2006-11-13  [6, 4, 389, 12839, 60, 1243, 129, 220, 1301, 1...  0.980159   \n",
       "2006-11-14  [6929, 41, 6, 4, 1763, 1297, 95, 241, 14, 77, ...  0.980159   \n",
       "...                                                       ...       ...   \n",
       "2013-11-10  [120, 1039, 3, 523, 9645, 12265, 10, 820, 848,...  0.009921   \n",
       "2013-11-11  [4936, 2, 59, 29, 977, 278, 934, 510, 66, 413,...  0.009921   \n",
       "2013-11-12  [295, 297, 2696, 1145, 1602, 529, 70, 1602, 55...  0.009921   \n",
       "2013-11-13  [467, 552, 1, 872, 269, 1972, 2, 1124, 2, 1103...  0.009921   \n",
       "2013-11-14  [533, 2877, 3055, 193, 71, 11, 232, 16, 318, 1...  0.011905   \n",
       "\n",
       "              3 YEAR       S&P  1 YEAR PRED  3 YEAR PRED  S&P PRED  \n",
       "DATE                                                                \n",
       "2006-11-10  0.909278  0.495985     0.975805     0.885219  0.486327  \n",
       "2006-11-11  0.921649  0.498327     0.978997     0.894900  0.487505  \n",
       "2006-11-12  0.905155  0.499378     0.979915     0.894458  0.488468  \n",
       "2006-11-13  0.904467  0.499215     0.981899     0.898712  0.489512  \n",
       "2006-11-14  0.903780  0.499051     0.975154     0.887344  0.497077  \n",
       "...              ...       ...          ...          ...       ...  \n",
       "2013-11-10  0.061856  1.000000     0.006076     0.068606  0.977189  \n",
       "2013-11-11  0.060481  0.998156     0.006066     0.068291  0.977625  \n",
       "2013-11-12  0.059107  0.996312     0.008256     0.074854  0.974523  \n",
       "2013-11-13  0.057732  0.994468     0.008017     0.066040  0.986115  \n",
       "2013-11-14  0.061856  0.991664     0.008066     0.056115  0.993311  \n",
       "\n",
       "[2556 rows x 7 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529, 500)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529, 20, 500)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1529 samples, validate on 506 samples\n",
      "1529/1529 [==============================] - 39s 26ms/sample - loss: 0.0283 - val_loss: 1.1881e-04\n"
     ]
    }
   ],
   "source": [
    "h = model.fit({'Text_Input': train_words, 'ARIMA_input': train_arima},\n",
    "              {'Main_Output': train_targets},\n",
    "              validation_data=({'Text_Input': val_words, 'ARIMA_input': val_arima},\n",
    "              {'Main_Output': val_targets}),\n",
    "              batch_size=par['batch_size'],\n",
    "              epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(h.history['loss'],color=\"blue\",label=\"Loss\")\n",
    "plt.plot(h.history['val_loss'],color=\"red\",label=\"Val_Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(range(5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and parameters\n",
    "date_str = date.today().strftime(\"%y%h%d-%Hh%Mm\")\n",
    "dir_path = Path(f\"./Models/model_del_{par['delay']}_t{date_str}\")\n",
    "os.mkdir(dir_path)\n",
    "model_path = dir_path / \"m.h5\"\n",
    "model.save(model_path)\n",
    "config_path = dir_path / \"config.pkl\"\n",
    "pickle.dump(par,open(config_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_dim': 100,\n",
       " 'vocab_part': 0.6,\n",
       " 'lookback': 20,\n",
       " 'delay': 5,\n",
       " 'batch_size': 10,\n",
       " 'p': 1,\n",
       " 'd': 1,\n",
       " 'q': 1,\n",
       " 'train_part': 0.6,\n",
       " 'val_part': 0.2,\n",
       " 'test_part': 0.2,\n",
       " 'series': '1 YEAR',\n",
       " 'input_dim': 500,\n",
       " 'start_date': Timestamp('2006-10-20 00:00:00'),\n",
       " 'end_date': Timestamp('2013-11-19 00:00:00'),\n",
       " 'vocab_size': 25664}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer lstm_9 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 500, 100]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-4921ff5fd662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# LSTM-layer over the embedding layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mlstm_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lookback'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'embed_dim'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 812\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    813\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    175\u001b[0m                          \u001b[1;34m'expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'. Full shape received: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                          str(x.shape.as_list()))\n\u001b[0m\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer lstm_9 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 20, 500, 100]"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network\n",
    "text_inputs = Input(shape=(par['lookback'],par['input_dim']), name='Text_Input')\n",
    "\n",
    "# Layer for word embedding\n",
    "embedded_layer = Embedding(input_dim=par['vocab_size'], \n",
    "                           output_dim=par['embed_dim'], # = 100 \n",
    "                           input_length=(par['input_dim']), name='Embedding_Layer')(text_inputs)\n",
    "\n",
    "# LSTM-layer over the embedding layer \n",
    "lstm_out = LSTM(10, input_shape=(par['lookback'],par['embed_dim']))(embedded_layer)\n",
    "dropout = Dropout(0.2)(lstm_out)\n",
    "\n",
    "# Input from an ARIMA-model independently fitted to the training data. \n",
    "ARIMA_input = Input(shape=(1,), name='ARIMA_input')\n",
    "\n",
    "# Merging the ARIMA-input and the input from the LSTM-layer. \n",
    "hidden = keras.layers.concatenate([dropout, ARIMA_input])\n",
    "\n",
    "# Stack of dense layers \n",
    "hidden = Dense(64, activation='relu',name='Dense_1')(hidden)\n",
    "hidden = Dropout(0.2)(hidden)\n",
    "\n",
    "hidden = Dense(64, activation='relu',name='Dense_2')(hidden)\n",
    "hidden = Dropout(0.2)(hidden)\n",
    "\n",
    "# Main output of the model\n",
    "main_output = Dense(1,activation='linear',name='Main_Output')(hidden)\n",
    "\n",
    "model = Model(inputs=[text_inputs, ARIMA_input],outputs=[main_output])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.fit({'Text_Input': train_words, 'ARIMA_input': train_arima},\n",
    "              {'Main_Output': train_targets},\n",
    "              validation_data=({'Text_Input': val_words, 'ARIMA_input': val_arima},\n",
    "              {'Main_Output': val_targets}),\n",
    "              batch_size=par['batch_size'],\n",
    "              epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
