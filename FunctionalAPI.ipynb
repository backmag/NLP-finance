{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import plot_mjodel\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from collections import deque\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "%run ARIMA-creator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_daily(df):\n",
    "    conc = pd.DataFrame()\n",
    "    for i in set(df.index):\n",
    "        concat_str = ''\n",
    "        for title in df.loc[i]['title']:\n",
    "            concat_str += \" \" + title\n",
    "        conc = conc.append({'date':i, 'title':concat_str},ignore_index=True)\n",
    "    conc.set_index('date',inplace=True)\n",
    "    conc = conc.sort_values('date')\n",
    "    return conc\n",
    "\n",
    "def pad_data(seq,maxlen):\n",
    "    data = np.zeros((len(seq), maxlen),dtype=int)\n",
    "    for i,s in enumerate(seq): \n",
    "        if len(s) <= maxlen: \n",
    "            data[i,:len(s)] = s\n",
    "        else: \n",
    "            s = np.array(s)\n",
    "            indices = np.sort(np.random.choice(len(s),maxlen,replace=False))\n",
    "            data[i,:] = s[indices]\n",
    "    return data\n",
    "\n",
    "def eval_preds(preds,targets): \n",
    "    \"\"\" Calculate the MSE of the ARIMA-predictions \n",
    "    and the actual prices \"\"\"\n",
    "    return (np.square(preds - targets)).mean(axis=0)\n",
    "\n",
    "def load_fasttext_matrix(fname, words): \n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    nbr_words = len(words)\n",
    "    embedding_matrix = np.zeros((nbr_words,300))\n",
    "    ctr = 0\n",
    "    not_found_words = list(words.values())\n",
    "    for line in fin: \n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if tokens[0] in words.keys(): \n",
    "            index = words.get(tokens[0])\n",
    "            embedding_matrix[index-1] = tokens[1:]\n",
    "            not_found_words.remove(index)\n",
    "            if len(not_found_words) == 0:\n",
    "                print(\"Found all words after {} %. \".format(100 * round(ctr / n,3)))\n",
    "                return embedding_matrix, not_found_words\n",
    "        ctr += 1\n",
    "    print(\"Found {} words. \".format(nbr_words - len(not_found_words)))\n",
    "    return embedding_matrix, not_found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters to be saved in a config file later. \n",
    "par = {\n",
    "    'embed_dim': 300,    # Dimensions to use for the word embedding\n",
    "    'vocab_part': 0.6,   # How large part of the total vocabulary to include\n",
    "    'lookback': 3,       # How far back to collect data in the recurrent layer (days)\n",
    "    'delay': 1,          # How far ahead to predict data (days)\n",
    "    'batch_size': 10,    # Batch size used in generator\n",
    "    'p': 1,              # Order of the AR-part of the model\n",
    "    'd': 1,              # Integrated order\n",
    "    'q': 1,              # Included moving average terms \n",
    "    'train_part' : 0.8,  # Part of data to be used for training\n",
    "    'val_part' : 0.1,    # Part of data to be used for validation\n",
    "    'test_part' : 0.1,   # Part of data to be used for testing\n",
    "    'series': '1 YEAR'   # What series we currently want to predict, '1 YEAR', '3 YEAR' or 'S&P'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "news_path = Path(os.getcwd()) / \"Datasets/data/financial_headlines_20061020-20131119.pkl\"\n",
    "stock_path = Path(os.getcwd()) / \"Datasets/data/stock_data.pkl\"\n",
    "data = pd.DataFrame(pd.read_pickle(news_path))\n",
    "data.set_index('date',inplace=True)\n",
    "stock_data = pd.DataFrame(pd.read_pickle(stock_path))\n",
    "data = concat_daily(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process text data \n",
    "\n",
    "len_words = [len(title) for title in data['title'].values]\n",
    "mean_words = np.mean(len_words)\n",
    "std_words = np.std(len_words)\n",
    "\n",
    "sent_len = int(mean_words + 2 * std_words)  # THIS MAKES IT SLOW!?                                                                                                                                                        \n",
    "sent_len = 500\n",
    "\n",
    "\n",
    "# Update environment variable config\n",
    "par.update({'input_dim': sent_len})\n",
    "par.update({'start_date' : data.index[0]})\n",
    "par.update({'end_date' : data.index[-1]})\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=None)  # Tokenize without limitation first, just because it's a cheap way \n",
    "tokenizer.fit_on_texts(data['title'])  # of calculating the number of unique words in the data\n",
    "par.update({'vocab_size': len(tokenizer.word_index)})\n",
    "\n",
    "tokenizer = Tokenizer(num_words=int(par['vocab_size'] * par['vocab_part']),filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\' ')\n",
    "\n",
    "tokenizer.fit_on_texts(data['title'])\n",
    "sequences = tokenizer.texts_to_sequences(data['title'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "par.update({'vocab_size': len(word_index)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAST_path = Path(os.getcwd() + \"/Embeddings/FASTtext/wiki-news-300d-1M.vec/wiki-news-300d-1M.vec\")\n",
    "FAST_embeddings, not_found = load_fasttext_matrix(FAST_path, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate random vectors for words not found in the pre-trained FASTtext-matrix\n",
    "for i in not_found: \n",
    "    FAST_embeddings[i - 1] = np.random.normal(scale=0.05,size=par['embed_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print not found words \n",
    "rev_word_index = {v:k for (k,v) in word_index.items()}\n",
    "[print(rev_word_index.get(i)) for i in not_found] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length sent_len. End-padded.\n",
    "# If a sequence is longer than sent_len, words are randomly sampled.\n",
    "text_data = pad_data(sequences,sent_len)\n",
    "\n",
    "# Extract financial data\n",
    "stock_data = stock_data[par['start_date'] : par['end_date']]\n",
    "\n",
    "# Add indicies which are present in the news data but not in the \n",
    "# financial data and interpolate missing values \n",
    "stock_data = stock_data.reindex(data.index.drop_duplicates())\n",
    "stock_data = stock_data.interpolate()\n",
    "\n",
    "# Normalize the financial data to [0,1]\n",
    "fin_stats = pd.DataFrame(columns=['min','max'])\n",
    "for col in stock_data: \n",
    "    minimum = min(stock_data[col])\n",
    "    maximum = max(stock_data[col])\n",
    "    fin_stats = fin_stats.append({'min':minimum, 'max':maximum},ignore_index=True)\n",
    "    stock_data[col] = [(row - minimum) / (maximum - minimum) for row in stock_data[col]]\n",
    "fin_stats.index = stock_data.columns\n",
    "\n",
    "# Concatinate all data to one dataframe \n",
    "data = pd.DataFrame()\n",
    "for text in text_data:\n",
    "    data = data.append({'WORDS':text},ignore_index=True)\n",
    "data['DATE'] = stock_data.index\n",
    "data.set_index('DATE',inplace=True)\n",
    "for col in stock_data: \n",
    "    data[col] = stock_data[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the ARIMA-models has been previously calculated for this config\n",
    "# or if they have to be constructed\n",
    "path = Path(f\"./Models/ARIMA/all_mods_del{par['lookback']}.pkl\")\n",
    "if os.path.exists(path):\n",
    "    # Load model if if already exists\n",
    "    ARIMA_models = pd.read_pickle(path)    \n",
    "    print(\"Found and loaded previously constructed models.\")\n",
    "else: \n",
    "    # Fit ARIMA-models to all of the dates in the training data \n",
    "    print(\"Model not found, fitting models\")\n",
    "    ARIMA_models = fit_all_models(par, data)\n",
    "    ARIMA_models.to_pickle(path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "arima_preds = predict_arima(ARIMA_models, par['delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_data = []\n",
    "sequence_dates = []\n",
    "prev_data = deque(maxlen=par['lookback'])\n",
    "for i,row in enumerate(data['WORDS']): \n",
    "    prev_data.append(row)\n",
    "    if len(prev_data) == par['lookback']:\n",
    "        sequential_data.append(np.array(prev_data))\n",
    "        sequence_dates.append(data.index[i])\n",
    "sequential_data = np.asarray(sequential_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the values not in arima_preds. This is just values in the beginning where\n",
    "# there isn't enough data to make a prediction, varies depending on lookback and delay. \n",
    "for date in data.index: \n",
    "    if not date in arima_preds.index : data.drop(index=date,inplace=True)\n",
    "        \n",
    "# Add the predicitons to the training data \n",
    "data['1 YEAR PRED'] = arima_preds['1 YEAR'].values\n",
    "data['3 YEAR PRED'] = arima_preds['3 YEAR'].values\n",
    "data['S&P PRED'] = arima_preds['S&P'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the target data (the actual rates) so that each row has a target rate 'delay' days in the future\n",
    "data['1 YEAR'] = data['1 YEAR'].shift(-par['delay'])\n",
    "data['3 YEAR'] = data['3 YEAR'].shift(-par['delay'])\n",
    "data['S&P'] = data['S&P'].shift(-par['delay'])\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_rows = []\n",
    "for i,date in enumerate(sequence_dates): \n",
    "    if date not in data.index: del_rows.append(i)\n",
    "keep_rows = np.setdiff1d(np.arange(len(sequential_data)),del_rows)\n",
    "sequential_data = sequential_data[keep_rows]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the data into training, validation and test segments. \n",
    "indices = np.arange(len(data))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Here we extract the time series specified in 'par' (for some reason...)\n",
    "words = np.array([row for row in data['WORDS'].values])\n",
    "arima_preds = np.array([row for row in data[par['series'] + ' PRED'].values])\n",
    "targets = np.array([row for row in data[par['series']].values])\n",
    "\n",
    "\n",
    "training_samples = round(par['train_part'] * len(data))\n",
    "validation_samples = round(par['val_part'] * len(data))\n",
    "test_samples = round(par['test_part'] * len(data))\n",
    "\n",
    "train_words = words[:training_samples - par['delay']]\n",
    "train_seq = sequential_data[:training_samples - par['delay']]\n",
    "train_arima = arima_preds[:training_samples - par['delay']]\n",
    "train_targets = targets[:training_samples - par['delay']]\n",
    "\n",
    "val_words = words[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_seq = sequential_data[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_arima = arima_preds[training_samples:training_samples + validation_samples - par['delay']]\n",
    "val_targets = targets[training_samples:training_samples + validation_samples - par['delay']]\n",
    "\n",
    "test_words = words[-test_samples:]\n",
    "test_seq = sequential_data[-test_samples:]\n",
    "test_arima = arima_preds[-test_samples:]\n",
    "test_targets = targets[-test_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to get the more conventional shape of (samples, sent_len, lookback)\n",
    "sequential_data = np.transpose(sequential_data, (0,2,1))\n",
    "sequential_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data \n",
    "train_indices = np.arange(len(train_targets))\n",
    "random.shuffle(train_indices)\n",
    "train_words = train_words[train_indices]\n",
    "train_seq = train_seq[train_indices]\n",
    "train_arima = train_arima[train_indices]\n",
    "train_targets = train_targets[train_indices]\n",
    "\n",
    "val_indices = np.arange(len(val_targets))\n",
    "val_words = val_words[val_indices]\n",
    "val_seq = val_seq[val_indices]\n",
    "val_arima = val_arima[val_indices]\n",
    "val_targets = val_targets[val_indices]\n",
    "\n",
    "test_indices = np.arange(len(test_targets))\n",
    "test_words = test_words[test_indices]\n",
    "test_seq = test_seq[test_indices]\n",
    "test_arima = test_arima[test_indices]\n",
    "test_targets = test_targets[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging folder for TensorBoard\n",
    "logdir=Path(\"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_words.shape)\n",
    "print(train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either create a new network or continue training a previous one. \n",
    "# Also specify the used embeddings here for use with all models \n",
    "model_embeddings = FAST_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "text_inputs = layers.Input(shape=(par['input_dim']), name='Text_Input')\n",
    "\n",
    "# Layer for word embedding\n",
    "embedded_layer = layers.Embedding(input_dim=par['vocab_size'],\n",
    "                           output_dim=par['embed_dim'],\n",
    "                           input_length=(par['input_dim']),\n",
    "                            weights=[model_embeddings], name='Embedding_Layer')(text_inputs)\n",
    "\n",
    "# LSTM-layer over the embedding layer \n",
    "lstm_out = layers.Bidirectional(layers.LSTM(10))(embedded_layer)\n",
    "dropout = layers.Dropout(0.2)(lstm_out)\n",
    "dense = layers.Dense(1)(dropout)\n",
    "\n",
    "# Input from an ARIMA-model independently fitted to the training data. \n",
    "ARIMA_input = layers.Input(shape=(1,), name='ARIMA_input')\n",
    "\n",
    "# Merging the ARIMA-input and the input from the LSTM-layer. \n",
    "hidden = layers.concatenate([dense, ARIMA_input])\n",
    "\n",
    "# Dense layers \n",
    "hidden = layers.Dense(2, activation='linear',name='Dense_1')(hidden)\n",
    "\n",
    "# Main output of the model\n",
    "main_output = layers.Dense(1,activation='linear',name='Main_Output')(hidden)\n",
    "\n",
    "model_word = Model(inputs=[text_inputs, ARIMA_input],outputs=[main_output])\n",
    "\n",
    "model_word.compile(optimizer='rmsprop',\n",
    "             loss='mse')\n",
    "\n",
    "\n",
    "model_word.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load the latest model (stored in model_path)\n",
    "#model_path = Path(os.getcwd()) / \"Models/NeuralNetworks/model_del_1_t20Mar12-17h11m/m_word.h5\"\n",
    "#model_word = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_model(model_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 10\n",
    "h = model_word.fit({'Text_Input': train_words, 'ARIMA_input': train_arima},\n",
    "              {'Main_Output': train_targets},\n",
    "              validation_data=({'Text_Input': val_words, 'ARIMA_input': val_arima},\n",
    "              {'Main_Output': val_targets}),\n",
    "              batch_size=par['batch_size'],\n",
    "              epochs=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(h.history['loss'],color=\"blue\",label=\"Loss\")\n",
    "plt.plot(h.history['val_loss'],color=\"red\",label=\"Val_Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(range(0,eps,int(eps / 10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and parameters\n",
    "date_str = date.today().strftime(\"%y%h%d-%Hh%Mm\")\n",
    "dir_path = Path(f\"./Models/NeuralNetworks/model_del_{par['delay']}_t{date_str}\")\n",
    "os.mkdir(dir_path)\n",
    "model_path = dir_path / \"m_word.h5\"\n",
    "model_word.save(model_path)\n",
    "config_path = dir_path / \"config.pkl\"\n",
    "pickle.dump(par,open(config_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time distributed model  \n",
    "\n",
    "# Initialize the neural network\n",
    "text_inputs = layers.Input(shape=(par['lookback'],par['input_dim']), name='Text_Input')\n",
    "\n",
    "# Layer for word embedding\n",
    "embedded_layer = layers.TimeDistributed(layers.Embedding(input_dim=par['vocab_size'],\n",
    "                           output_dim=par['embed_dim'],\n",
    "                           input_length=par['input_dim'],\n",
    "                            weights=[model_embeddings],\n",
    "                                name='Embedding_Layer'))(text_inputs)\n",
    "\n",
    "embedded_layer = layers.TimeDistributed(Flatten())(embedded_layer)\n",
    "\n",
    "# LSTM-layer over the embedding layer \n",
    "lstm_out = layers.Bidirectional(layers.LSTM(5, input_shape=(None,par['lookback'],par['embed_dim'])))(embedded_layer)\n",
    "dropout = layers.Dropout(0.2)(lstm_out)\n",
    "dense = layers.Dense(1)(dropout)\n",
    "\n",
    "# Input from an ARIMA-model independently fitted to the training data. \n",
    "ARIMA_input = layers.Input(shape=(1,), name='ARIMA_input')\n",
    "\n",
    "# Merging the ARIMA-input and the input from the LSTM-layer. \n",
    "hidden = layers.concatenate([dense, ARIMA_input])\n",
    "\n",
    "# Stack of dense layers \n",
    "hidden = layers.Dense(2, activation='linear',name='Dense_1')(hidden)\n",
    "\n",
    "\n",
    "# Main output of the model\n",
    "main_output = layers.Dense(1,activation='linear',name='Main_Output')(hidden)\n",
    "\n",
    "model_p = Model(inputs=[text_inputs, ARIMA_input],outputs=[main_output])\n",
    "\n",
    "model_p.compile(optimizer='rmsprop',\n",
    "             loss='mse')\n",
    "\n",
    "model_p.summary()\n",
    "\n",
    "plot_model(model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eps = 10\n",
    "h_p = model_p.fit({'Text_Input': train_seq, 'ARIMA_input': train_arima},\n",
    "              {'Main_Output': train_targets},\n",
    "              validation_data=({'Text_Input': val_seq, 'ARIMA_input': val_arima},\n",
    "              {'Main_Output': val_targets}),\n",
    "              batch_size=par['batch_size'],\n",
    "              epochs=eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_p.history['loss'],color=\"blue\",label=\"Loss\")\n",
    "plt.plot(h_p.history['val_loss'],color=\"red\",label=\"Val_Loss\")\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.xticks(range(0,eps,int(eps / 10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima model which just outputs the input, convenient for testing and comparing \n",
    "inputs = layers.Input(shape=(1,), name='ARIMA_Input')\n",
    "output = layers.Dense(1, activation='linear',name='Output')(inputs)\n",
    "\n",
    "model_dummy_arima = Model(inputs=[inputs],outputs=[output])\n",
    "model_dummy_arima.layers[-1].set_weights([np.array(1).reshape(1,1), np.array(0.0).reshape(1)])\n",
    "model_dummy_arima.layers[-1].trainable = False\n",
    "model_dummy_arima.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "#arima_dummy_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and compare to the ARIMA-predictions\n",
    "\n",
    "train_mse_nlp_seq = model_p.evaluate({'Text_Input': train_seq, 'ARIMA_input': train_arima},\n",
    "                {'Main_Output': train_targets},verbose=0)\n",
    "train_mse_nlp_word = model_word.evaluate({'Text_Input': train_words, 'ARIMA_input': train_arima},\n",
    "                {'Main_Output': train_targets},verbose=0)\n",
    "train_mse_arima = model_dummy_arima.evaluate(train_arima, train_targets,verbose=0)\n",
    "print(\" ------ TRAIN RESULTS ------ \")\n",
    "print(\"NLP MSE WORDS: \",train_mse_nlp_word)\n",
    "print(\"NLP MSE SEQUENCE: \",train_mse_nlp_seq)\n",
    "print(\"ARIMA MSE: \",train_mse_arima)\n",
    "print(\"Percentage MSE word vs arima: {0:+} %\".format(round(100 * (train_mse_nlp_word - train_mse_arima) / train_mse_nlp_word,3)))\n",
    "print(\"Percentage MSE seq vs arima: {0:+} %\".format(round(100 * (train_mse_nlp_seq - train_mse_arima) / train_mse_nlp_seq,3)))\n",
    "\n",
    "val_mse_nlp_seq = model_p.evaluate({'Text_Input': val_seq, 'ARIMA_input': val_arima},\n",
    "                {'Main_Output': val_targets},verbose=0)\n",
    "val_mse_nlp_word = model_word.evaluate({'Text_Input': val_words, 'ARIMA_input': val_arima},\n",
    "                {'Main_Output': val_targets},verbose=0)\n",
    "val_mse_arima = model_dummy_arima.evaluate(val_arima, val_targets,verbose=0)\n",
    "print(\" ------ VAL RESULTS ------ \")\n",
    "print(\"NLP MSE WORD: \",val_mse_nlp_word)\n",
    "print(\"NLP MSE SEQUENCE: \",val_mse_nlp_seq)\n",
    "print(\"ARIMA MSE: \",val_mse_arima)\n",
    "print(\"Percentage MSE word vs arima: {0:+} %\".format(round(100 * (val_mse_nlp_word - val_mse_arima) / val_mse_nlp_word,3)))\n",
    "print(\"Percentage MSE seq vs arima: {0:+} %\".format(round(100 * (val_mse_nlp_seq - val_mse_arima) / val_mse_nlp_seq,3)))\n",
    "\n",
    "test_mse_nlp_seq = model_p.evaluate({'Text_Input': test_seq, 'ARIMA_input': test_arima},\n",
    "                {'Main_Output': test_targets},verbose=0)\n",
    "test_mse_nlp_word = model_word.evaluate({'Text_Input': test_words, 'ARIMA_input': test_arima},\n",
    "                {'Main_Output': test_targets},verbose=0)\n",
    "\n",
    "test_mse_arima = model_dummy_arima.evaluate(test_arima, test_targets,verbose=0)\n",
    "\n",
    "print(\" ------ TEST RESULTS ------ \")\n",
    "print(\"NLP MSE WORD: \",test_mse_nlp_word)\n",
    "print(\"NLP MSE SEQUENCE: \",test_mse_nlp_seq)\n",
    "print(\"ARIMA MSE: \",test_mse_arima)\n",
    "print(\"Percentage MSE word vs arima: {0:+} %\".format(round(100 * (test_mse_nlp_word - test_mse_arima) / test_mse_nlp_word,3)))\n",
    "print(\"Percentage MSE seq vs arima: {0:+} %\".format(round(100 * (test_mse_nlp_seq - test_mse_arima) / test_mse_nlp_seq,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p.layers[-1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
