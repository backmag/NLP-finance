{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, Flatten, Layer, Lambda, LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "from IPython.display import Image\n",
    "from collections import deque\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "from importnb import Notebook, reload\n",
    "with Notebook(): \n",
    "    import Utilities\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    \"\"\" Tokenize \"\"\"\n",
    "    conc = pd.DataFrame()\n",
    "    for i in set(df.index):\n",
    "        concat_str = ''\n",
    "        for title in df.loc[i]['title']:\n",
    "            concat_str += \" \" + title.lower()\n",
    "        concat_str = word_tokenize(concat_str)\n",
    "        conc = conc.append({'date':i, 'title':concat_str},ignore_index=True)\n",
    "    conc.set_index('date',inplace=True)\n",
    "    conc = conc.sort_values('date')\n",
    "    sents = [''.join([word + ' ' for word in title]) for title in conc['title'].values]\n",
    "    return sents\n",
    "\n",
    "def eval_preds(preds,targets): \n",
    "    \"\"\" Calculate the MSE of the ARIMA-predictions \n",
    "    and the actual prices \"\"\"\n",
    "    return (np.square(preds - targets)).mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embed_dim': 300,    # Dimensions to use for the word embedding\n",
    "    'lookback': 3,       # How far back to collect data in the recurrent layer (days)\n",
    "    'delay': 1,          # How far ahead to predict data (days)\n",
    "    'batch_size': 10,    # Batch size used in generator\n",
    "    'p': 1,              # Order of the AR-part of the model\n",
    "    'd': 1,              # Integrated order\n",
    "    'q': 1,              # Included moving average terms \n",
    "    'train_part' : 0.8,  # Part of data to be used for training\n",
    "    'val_part' : 0.1,    # Part of data to be used for validation\n",
    "    'test_part' : 0.1,   # Part of data to be used for testing\n",
    "    'series': '1 YEAR',   # What series we currently want to predict, '1 YEAR', '3 YEAR' or 'S&P'\n",
    "    'vocab_size': 15000,  # Include only the 'vocab_size' most common words \n",
    "    'start_date': '2006-10-20',\n",
    "    'end_date': '2013-11-19',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "news_path = Path(os.getcwd()) / \"Datasets/data/financial_headlines_20061020-20131119.pkl\"\n",
    "stock_path = Path(os.getcwd()) / \"Datasets/data/stock_data.pkl\"\n",
    "data = pd.DataFrame(pd.read_pickle(news_path))\n",
    "data.set_index('date',inplace=True)\n",
    "stock_data = pd.DataFrame(pd.read_pickle(stock_path))\n",
    "text = preprocess_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data \n",
    "tokenizer = Tokenizer(num_words=config['vocab_size'])\n",
    "tokenizer.fit_on_texts(text)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dictionary of embeddings for this vocabulary (previously constructed)\n",
    "subset_embeddings = Path(os.getcwd()) / \"Embeddings/GloVe/saved.42B.300d.pkl\"\n",
    "with open(subset_embeddings,'rb') as handle: \n",
    "    emb_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix with shape (vocab_size, embed_dimension)\n",
    "embedding_weights = np.zeros((config['vocab_size'], config['embed_dim']))\n",
    "# Add pre-trained weights from GloVe\n",
    "for word,index in word_index.items(): \n",
    "    if index > config['vocab_size']: \n",
    "        break\n",
    "    temp_emb = emb_dict.get(word)\n",
    "    if temp_emb is not None: \n",
    "        embedding_weights[index - 1] = temp_emb\n",
    "    else: \n",
    "        embedding_weights[index - 1] = np.random.normal(size=config['embed_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual maximum length of one day news: 950\n",
      "Used maximum length of one day news: 600\n"
     ]
    }
   ],
   "source": [
    "config['max_len'] = max([len(sent) for sent in sequences])\n",
    "print(\"Actual maximum length of one day news:\",config['max_len'])\n",
    "config['max_len'] = 600\n",
    "print(\"Used maximum length of one day news:\",config['max_len'])\n",
    "\n",
    "# Pad the text data so we get a matrix of shape (n,d)\n",
    "x = np.zeros((len(sequences), config['max_len']))\n",
    "for i,sent in enumerate(sequences): \n",
    "    if len(sent) > config['max_len']: \n",
    "        x[i] = sent[:config['max_len']]\n",
    "    else: \n",
    "        x[i,:len(sent)] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and loaded previously constructed models.\n"
     ]
    }
   ],
   "source": [
    "y = Utilities.load_financial_labels(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train:  (2060, 600)\n",
      "Shape of y_train:  (2060,)\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train), (x_test, y_test) = Utilities.shuffle_and_partition(x,y, test_part=0.2)\n",
    "print(\"Shape of x_train: \",x_train.shape)\n",
    "print(\"Shape of y_train: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model \n",
    "This model aims to take the chronology of news over a period into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some help functions for custom layer and hyper parameter optimization\n",
    "\n",
    "# Create functions for custom 'MergeEmedding'-layer which averages \n",
    "# the embeddings over all words after the embedding layer \n",
    "def merge_embeddings(x):\n",
    "    # Sum the embeddings for every word slot. If this is zero, there is no word in this slot\n",
    "    non_zero = K.sum(K.cast(K.not_equal(K.sum(x,axis=2),0),tf.float32))\n",
    "    return K.sum(x,axis=1) / non_zero\n",
    "\n",
    "def merge_output_shape(input_shape):\n",
    "    return input_shape.pop(2)\n",
    "    #return input_shape[0], input_shape[1], input_shape[3]\n",
    "# OR \n",
    "# return input_shape.pop(2)\n",
    "\n",
    "\n",
    "def train_and_format(x,y,bs,epochs, results, layers, nodes,verbose=0):\n",
    "    name = f\"{bs}_{layers}_{nodes}\"\n",
    "    temp_history = model.fit(x, y, batch_size=bs, validation_split=0.2, epochs = epochs,verbose=verbose)\n",
    "    return results.append(pd.Series([np.mean(temp_history.history['val_loss'][-5:]),\n",
    "                                                temp_history, model, layers, \n",
    "                                                nodes, bs], \n",
    "                                                name=name,index=['Mean Val MSE','History','Model','Layers','Nodes','Batch Size']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 3, 600)]          0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 3, 600, 300)       4500000   \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 600, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 600, 5)            6120      \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 5)                 220       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 4,506,346\n",
      "Trainable params: 4,506,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " \n",
    "input_layer = Input(shape=(config['lookback'],config['max_len']), name='input')\n",
    "embed_layer = Embedding(input_dim=config['vocab_size'],\n",
    "                output_dim=config['embed_dim'],\n",
    "                weights=[embedding_weights])(input_layer)\n",
    "merged_layer = Lambda(merge_embeddings, output_shape=merge_output_shape)(embed_layer)\n",
    "lstm_layer = LSTM(5,return_sequences=True)(merged_layer)\n",
    "lstm_layer = LSTM(5,return_sequences=False)(lstm_layer)\n",
    "output_layer = Dense(1, activation='linear', name='output')(lstm_layer)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
