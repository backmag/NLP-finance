{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(path,method=\"tfidf\",period=1,max_words=1000):\n",
    "    \"\"\" Process a text into some word representation, bag of words, tfidf, word2vec etc. \n",
    "\n",
    "    Parameters: \n",
    "    path - The path to the data. Should be a pandas df saved as a .pkl-file, indexed by datetime. \n",
    "    method - The method for tokenizing, one of the following: ['bow', 'tfidf', 'embed', 'attention']\n",
    "    period - The period to concatinate titles in expressed in days, at least 1 day. \n",
    "\n",
    "    Returns: \n",
    "    A numpy array (tokens) and the indices expressed as dates in datetime format. \n",
    "    \"\"\"\n",
    "    \n",
    "    news_data = pd.DataFrame(pd.read_pickle(path))\n",
    "    news_data.set_index('date',inplace=True)\n",
    "    print(\"Found {} days with {} news titles.\".format(len(set(news_data.index)), len(news_data.index)))\n",
    "    tokens, data = tokenize(news_data,max_words,method,period)\n",
    "    return tokens, data.index\n",
    "\n",
    "def concat_news(news,period):\n",
    "    \"\"\"Merges news headlines over a period of days as one single list of words,\n",
    "    tokenized by word_tokenize() in nltk. \n",
    "    \n",
    "    Parameters: \n",
    "    news (DataFrame): News data of headlines (str) indexed by datetime.   \n",
    "    period (int): Number of days in each period. \n",
    "    \n",
    "    Returns: \n",
    "    DataFrame of lists of words indexed by datetime.\n",
    "    \"\"\"\n",
    "    delta = timedelta(days=period)\n",
    "    t1 = news.index[0]\n",
    "    t2 = t1 + delta\n",
    "    end_date = news.index[-1]\n",
    "    data = pd.DataFrame({'titles': [], 'date': []})\n",
    "    all_titles = news['title'].apply(word_tokenize)\n",
    "    while(t2 < end_date):\n",
    "        period_words = []\n",
    "        titles = all_titles[t1.strftime(\"%Y-%m-%d\") : t2.strftime(\"%Y-%m-%d\")]\n",
    "        for title in titles: \n",
    "            for word in title: \n",
    "                period_words.append(word)\n",
    "        data = data.append({'date': t1, 'titles': gensim.utils.simple_preprocess(' '.join(period_words))},ignore_index=True)\n",
    "        t1 = t2\n",
    "        t2 = t2 + delta    \n",
    "    titles = list([word_tokenize(t) for t in news.loc[t1.strftime(\"%Y-%m-%d\") : end_date.strftime(\"%Y-%m-%d\")]['title']])\n",
    "    for title in titles: \n",
    "            for word in title: \n",
    "                period_words.append(word)\n",
    "    data = data.append({'date': t1, 'titles': period_words},ignore_index=True)\n",
    "    data.set_index('date',inplace=True)\n",
    "    return data\n",
    "\n",
    "def tokenize(news_data, max_words,method='tfidf',period=1):\n",
    "    data = concat_news(news_data, period) \n",
    "    print(\"Concatinated to {} samples with an average of {} titles per sample.\".format(len(data.index),round(len(news_data.index) / len(data.index),4)))\n",
    "    if method == 'tfidf':    \n",
    "        tokenizer = Tokenizer(num_words = max_words)\n",
    "        tokenizer.fit_on_texts(data['titles'])\n",
    "        tokens = tokenizer.texts_to_matrix(data['titles'],mode='tfidf')\n",
    "        return tokens, data\n",
    "    elif method == 'word2vec'\n",
    "        print(\"Not yet implemented.\")\n",
    "        from gensim.models import word2vec\n",
    "        processed_sents = [list(gensim.utils.tokenize(line,lowercase=True,deacc=True)) for line in news_data['title']]\n",
    "        w2v = Word2Vec(processed_sents, size=150, iter=10) \n",
    "    else: \n",
    "        print(\"Unknown tokenizing method.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = r\"./Datasets/data/financial_headlines_20061020-20131119.pkl\"\n",
    "#tokens = process_text(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
