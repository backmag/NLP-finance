{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tqdm import trange\n",
    "import datetime\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONEDRIVE_PATH = Path(r\"C:\\Users\\gusta\\Kidbrooke Advisory Ab\\KidbrookeOneDrive - Gustaf Backman exjobb\")\n",
    "\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_path = ONEDRIVE_PATH / 'Benchmark Datasets/IMDB Dataset.csv'\n",
    "imdb = pd.read_csv(imdb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "bert_module =  hub.Module(bert_path)\n",
    "tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                      tokenization_info[\"do_lower_case\"],])\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 50000/50000 [02:40<00:00, 311.29it/s]\n"
     ]
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "# Remove line breaks (<br />) and tokenize\n",
    "x = np.zeros((len(imdb), max_length))\n",
    "input_ids = np.zeros((len(imdb),max_length))\n",
    "input_masks = np.zeros((len(imdb),max_length))\n",
    "segment_ids = np.zeros((len(imdb),max_length))\n",
    "\n",
    "for i in trange(len(imdb)):\n",
    "    text = imdb.iloc[i]['review'].replace('<br />', ' ').lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > max_length - 2:\n",
    "        tokens = tokens[:max_length - 2]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "    if len(token_ids) > max_length:\n",
    "        token_ids = token_ids[:max_length]\n",
    "        mask = [1] * max_length\n",
    "    else: \n",
    "        mask = [1] * len(token_ids) + [0] * (max_length - len(token_ids))\n",
    "        token_ids = token_ids + [0] * (max_length - len(token_ids))\n",
    "    \n",
    "    current_segment_id = 0\n",
    "    segments = []\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == '[SEP]':\n",
    "            current_segment_id = 1\n",
    "    while len(segments) < max_length: \n",
    "        segments.append(0)\n",
    "    assert len(token_ids) == max_length, \"Wrong length of ids\"\n",
    "    assert len(segments) == max_length, \"Wrong length of segments.\"\n",
    "    assert len(mask) == max_length, \"Wrong length of masks.\"\n",
    "    \n",
    "    input_ids[i] = np.asarray(token_ids)\n",
    "    segment_ids[i] = segments\n",
    "    input_masks[i] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50000/50000 [00:04<00:00, 11703.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# Assign labels to variables for convenience\n",
    "y = np.zeros((50000))\n",
    "for i in trange(len(y)): \n",
    "    if imdb.iloc[i]['sentiment'] == 'positive':\n",
    "        y[i] = 1\n",
    "\n",
    "train_ids = input_ids[:20000]\n",
    "train_seg = segment_ids[:20000]\n",
    "train_mask = input_masks[:20000]\n",
    "\n",
    "val_ids = input_ids[20000:25000]\n",
    "val_seg = segment_ids[20000:25000]\n",
    "val_mask = input_masks[20000:25000]\n",
    "\n",
    "test_ids = input_ids[25000:]\n",
    "test_seg = segment_ids[25000:]\n",
    "test_mask = input_masks[25000:]\n",
    "\n",
    "y_train = y[:20000]\n",
    "y_val = y[20000:25000]\n",
    "y_test = y[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\nlpenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\nlpenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\nlpenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\nlpenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_id (InputLayer)           [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment (InputLayer)      [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_id[0][0]                   \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 input_segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 7,875,585\n",
      "Non-trainable params: 102,426,426\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "in_ids = keras.layers.Input(shape=(max_length,), dtype=tf.int32,name='input_id')\n",
    "in_mask = keras.layers.Input(shape=(max_length,),dtype=tf.int32,name='input_mask')\n",
    "in_seg = keras.layers.Input(shape=(max_length,),dtype=tf.int32, name='input_segment')\n",
    "\n",
    "bert_inputs = [in_ids, in_mask, in_seg]\n",
    "bert_output = BertLayer(n_fine_tune_layers=1,pooling='first')(bert_inputs)\n",
    "\n",
    "dense_layer = keras.layers.Dense(256, activation='relu')(bert_output)\n",
    "dropout_layer = keras.layers.Dropout(0.1)(dense_layer)\n",
    "\n",
    "output_layer = keras.layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "model = keras.models.Model(inputs=bert_inputs, outputs=output_layer)\n",
    "\n",
    "adam = keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.local_variables_initializer())\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "K.set_session(sess)\n",
    "\n",
    "# Dictionary for storing stats\n",
    "history = {'loss': [],\n",
    "          'acc': [],\n",
    "          'val_loss': [],\n",
    "          'val_acc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"Model Weights/date_0514_epochs_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "20000/20000 [==============================] - 27971s 1s/sample - loss: 0.1867 - acc: 0.9282 - val_loss: 0.2267 - val_acc: 0.9138\n"
     ]
    }
   ],
   "source": [
    "temp_hist = model.fit([train_ids, train_mask, train_seg], y_train,\n",
    "                      validation_data=([val_ids, val_mask, val_seg], y_val), \n",
    "                      epochs=2,                                                                                                                   \n",
    "                      batch_size=32)\n",
    "\n",
    "for metric in history: \n",
    "    history[metric].append(temp_hist.history[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"Model Weights/date_\" + datetime.date.today().strftime(\"%m%d\") + \"_epochs_3.h5\"\n",
    "model.save_weights(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "20000/20000 [==============================] - 23349s 1s/sample - loss: 0.1621 - acc: 0.9394 - val_loss: 0.2538 - val_acc: 0.9022\n"
     ]
    }
   ],
   "source": [
    "temp_hist = model.fit([train_ids, train_mask, train_seg], y_train,\n",
    "                      validation_data=([val_ids, val_mask, val_seg], y_val), \n",
    "                      epochs=1,                                                                                                                   \n",
    "                      batch_size=32)\n",
    "\n",
    "for metric in history: \n",
    "    history[metric].append(temp_hist.history[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"Model Weights/date_\" + datetime.date.today().strftime(\"%m%d\") + \"_epochs_4.h5\"\n",
    "model.save_weights(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 27214s 1s/sample - loss: 0.2337 - acc: 0.9098\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate([test_ids, test_mask, test_seg], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
